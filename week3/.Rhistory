filter(Country=="India") %>%
select(Votes, Switch.to.order.menu, Has.Table.booking, Has.Online.delivery) %>%
tidyr::gather(type, value, 2:4) %>%
mutate(type=gsub("[.]"," ",type)) %>%
group_by(type, value) %>%
summarise(mean_votes = mean(Votes,na.rm=TRUE))
df %>%
filter(Country=="India") %>%
select(Votes, Switch.to.order.menu, Has.Table.booking, Has.Online.delivery) %>%
tidyr::gather(type, value, 2:4) %>%
mutate(type=gsub("[.]"," ",type)) %>%
group_by(type, value) %>%
summarise(mean_votes = mean(Votes,na.rm=TRUE)) %>%
ggplot(aes(x=value,y=mean_votes)) +
geom_bar(stat='identity',fill='#cb202d') +
facet_wrap(~type) +
labs(x='Answer', y='Average Number of Votes',
title='Average Votes for different kinds of restaurants') +
theme(plot.title = element_text(hjust=0.5,face='bold',color='#2d2d2d'),
panel.background = element_blank(),
strip.text = element_text(face='bold',color='#2d2d2d'),
axis.text.x = element_text(face='bold',color='#2d2d2d'),
axis.text = element_text(face='bold',color='#2d2d2d'))
df %>%
filter(Country=="India") %>%
select(Votes, Switch.to.order.menu, Has.Table.booking, Has.Online.delivery) %>%
tidyr::gather(type, value, 2:4) %>%
mutate(type=gsub("[.]"," ",type)) %>%
group_by(type, value) %>%
summarise(mean_votes = mean(Votes,na.rm=TRUE)) %>%
ggplot(aes(x=value,y=mean_votes)) +
geom_bar(stat='identity',fill='#cb202d') +
facet_wrap(~type) +
labs(x='Answer', y='Average Number of Votes',
title='Average Votes for different kinds of restaurants') +
theme(plot.title = element_text(hjust=0.5,face='bold',color='#2d2d2d'),
panel.background = element_blank(),
strip.text = element_text(face='bold',color='#2d2d2d'),
axis.text.x = element_text(face='bold',color='#2d2d2d'),
axis.text = element_text(face='bold',color='#2d2d2d'))
source('C:/Users/User/Desktop/Data-Science-Programming/week3/TF-IDF_poem.R', encoding = 'UTF-8', echo=TRUE)
tokens
tokens
tokens
source('C:/Users/User/Desktop/Data-Science-Programming/week3/TF-IDF_poem.R', encoding = 'UTF-8', echo=TRUE)
seg
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/") )
# 載入所需的套件包
library(tm)
library(tmcn)
library(Matrix)
library(wordcloud)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(NLP)
library(jiebaRD)
library(jiebaR)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
d
d.corpus
View(d)
# 載入所需的套件包
library(tm)
library(tmcn)
library(Matrix)
library(wordcloud)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(NLP)
library(jiebaRD)
library(jiebaR)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
View(seg)
View(jieba_tokenizer)
View(d.corpus)
View(d.corpus)
d.corpus[["1"]]
d.corpus[["2"]]
d.corpus[1]
d.corpus[["5"]]
d[[1]]
d.corpus[[1]]
# 載入所需的套件包
library(tm)
library(tmcn)
library(Matrix)
library(wordcloud)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(NLP)
library(jiebaRD)
library(jiebaR)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/aa/") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
#計數
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
tokens
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/aa") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
#計數
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
tokens
# 載入所需的套件包
library(tm)
library(tmcn)
library(Matrix)
library(wordcloud)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(NLP)
library(jiebaRD)
library(jiebaR)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
#計數
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/aa") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
#計數
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
tokens
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
library(dplyr)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
gsub("[A-Za-z0-9]", "", word)
})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/") )
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
#d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
#  gsub("[A-Za-z0-9]", "", word)
#})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/") )
d.corpus <- tm_map(d.corpus, segmentCN)
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
#d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
#  gsub("[A-Za-z0-9]", "", word)
#})
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/") )
d.corpus <- tm_map(d.corpus, segmentCN)
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
#d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
#  gsub("[A-Za-z0-9]", "", word)
#})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
View(d.corpus)
View(seg)
View(tokens)
View(tokens)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/") )
#d.corpus <- tm_map(d.corpus, segmentCN)
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
#d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
#  gsub("[A-Za-z0-9]", "", word)
#})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
View(seg)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/") )
#d.corpus <- tm_map(d.corpus, segmentCN)
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
#d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
#  gsub("[A-Za-z0-9]", "", word)
#})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
View(seg)
View(d.corpus)
docs <- readLines("./data/poem/2018-07-23 dufu.txt")
docs
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/"), list(language = NA) )
#d.corpus <- tm_map(d.corpus, segmentCN)
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
#d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
#  gsub("[A-Za-z0-9]", "", word)
#})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
View(seg)
d.corpus <- Corpus( DirSource("./data/poem/"), list(language = NA) )
docs.seg <- tm_map(d.corpus, segmentCN)
docs.tdm <- TermDocumentMatrix(docs.seg)
View(docs.tdm)
View(docs.seg)
docs.seg[["1"]]
docs.tf <- apply(as.matrix(docs.tdm), 2, function(word) { word/sum(word) })
idf <- function(doc) {
return ( log2( length(doc)+1 / nnzero(doc)) )
}
docs.idf <- apply(as.matrix(docs.tdm), 1, idf)
docs.tfidf <- docs.tf * docs.idf
inspect(docs.tdm)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/"),encoding = "UTF-8" )
#d.corpus <- tm_map(d.corpus, segmentCN)
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
#d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
#  gsub("[A-Za-z0-9]", "", word)
#})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
View(seg)
View(d.corpus)
# 建立文本資料結構與基本文字清洗
d.corpus <- Corpus( DirSource("./data/poem/"),encoding = "UTF-8" )
#d.corpus <- tm_map(d.corpus, segmentCN)
d.corpus<-tm_map(d.corpus,stripWhitespace) #消除空格
d.corpus <- tm_map(d.corpus, removePunctuation) #移除標點符號
d.corpus <- tm_map(d.corpus, removeNumbers) #移除數字
#d.corpus <- tm_map(d.corpus, function(word) { # 移除大小寫
#  gsub("[A-Za-z0-9]", "", word)
#})
# 進行斷詞，並依照日期建立文本矩陣 TermDocumentMatrix
mixseg = worker()
#斷詞
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
View(seg)
library(tm)
library(tmcn)
library(Matrix)
library(wordcloud)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(NLP)
library(jiebaRD)
library(jiebaR)
d.corpus <- Corpus( DirSource("./data/aa") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
colNames <- gsub(".txt", "", colNames)
for( id in c(2:n) )
{
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM[is.na(TDM)] <- 0
library(knitr)
kable(head(TDM))
View(seg)
View(TDM)
View(tokens)
docs <- readLines("./data/aa/00.txt")
docs <- gsub("\\[[0-9]+\\]", "", docs)
d.corpus <- Corpus(VectorSource(docs))
#d.corpus <- Corpus( DirSource("./data/aa/00.txt") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
View(d.corpus)
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
View(seg)
seg
docs <- readLines("./data/aa/00.txt", ,encoding = "UTF-8")
docs <- gsub("\\[[0-9]+\\]", "", docs)
d.corpus <- Corpus(VectorSource(docs))
docs
docs <- readLines("./data/aa/00.txt")
docs <- gsub("\\[[0-9]+\\]", "", docs)
d.corpus <- Corpus(VectorSource(docs))
docs
docs <- readLines("./data/aa/00.txt")
docs <- gsub("\\[[0-9]+\\]", "", docs)
d.corpus <- Corpus(VectorSource(docs))
#d.corpus <- Corpus( DirSource("./data/aa/00.txt") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
